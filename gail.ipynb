{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp7k9bDBWIAY"
      },
      "source": [
        "# Group project for CS8803-DRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSPPXa3c0VK8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "# os.environ[\"MUJOCO_GL\"] = \"omesa\"  # Alternatively, try \"osmesa\" if you still face issues\n",
        "# os.environ[\"MUJOCO_PY_MUJOCO_PATH\"] = \"/Users/uzair/.mujoco/mujoco210\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPZ_zUtz0GcL",
        "outputId": "affe58bc-a36a-4208-93e9-1c3d0fce3f1d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import random\n",
        "import enlighten\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import gymnasium as gym\n",
        "from loguru import logger\n",
        "from copy import deepcopy\n",
        "from torch.optim import Adam\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions import categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# random seeds for reproducability\n",
        "SEED: int=42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "logger.info(f'Random seed set as {SEED}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9UlHEWL0PwN",
        "outputId": "69d7c063-6994-4ba4-b8c0-b4fa2d1a8110"
      },
      "outputs": [],
      "source": [
        "ENV_NAME: str=\"HalfCheetah-v4\"\n",
        "N_OBS: int=17\n",
        "N_ACTIONS: int=6\n",
        "#@title Device check\n",
        "def test_device_is_gpu():\n",
        "    if(\"cuda\" in device.type):\n",
        "        logger.info(f\"Test passed: Device is GPU!.\")\n",
        "    else:\n",
        "        logger.info(f\"Test failed: Device is not GPU! Continuing with CPU.\")\n",
        "\n",
        "# Run the test\n",
        "test_device_is_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS4YzED3ak-z"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameters\n",
        "EPOCHS: int=512\n",
        "GAMMA: float=0.99\n",
        "BATCH_SIZE: int=64\n",
        "HIDDEN_DIM: int=64\n",
        "EVAL_EPISODES: int=16\n",
        "GAE_LAMBDA: float=0.95\n",
        "PPO_EPSILON: float=0.2\n",
        "LEARNING_RATE: float=4e-4\n",
        "GENERATOR_ITERATIONS: int=32\n",
        "DISCRIMINATOR_ITERATIONS: int=1\n",
        "# I've defined this function here to reduce code duplicaiton\n",
        "def make_network(in_dim, out_dim, hidden_dim=HIDDEN_DIM, device=device):\n",
        "    \"\"\"\n",
        "    Returns a NN with the specified dimensions.\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, out_dim)\n",
        "    ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZLOSpsR0bP5"
      },
      "outputs": [],
      "source": [
        "# Let's build upon the course PPO implementation\n",
        "class TrajData:\n",
        "    def __init__(self, n_steps, n_envs, n_obs, n_actions):\n",
        "        s, e, o, a = n_steps, n_envs, n_obs, n_actions\n",
        "        from torch import zeros\n",
        "\n",
        "        self.states = zeros((s, e, o), device=device)\n",
        "        self.actions = zeros((s, e, a), device=device)\n",
        "        self.rewards = zeros((s, e), device=device)\n",
        "        self.not_dones = zeros((s, e), device=device)\n",
        "\n",
        "        self.log_probs = zeros((s, e), device=device)\n",
        "        self.returns = zeros((s, e), device=device)\n",
        "        self.advantages = zeros((s, e), device=device)\n",
        "\n",
        "        self.n_steps = s\n",
        "\n",
        "    def detach(self):\n",
        "        self.actions = self.actions.detach()\n",
        "        self.log_probs = self.log_probs.detach()\n",
        "\n",
        "    def store(self, t, s, a, r, lp, d):\n",
        "        self.states[t] = torch.tensor(s, dtype=torch.float, device=device)\n",
        "        self.actions[t] = a.to(device)\n",
        "        self.rewards[t] = torch.tensor(r, dtype=torch.float, device=device)\n",
        "\n",
        "        self.log_probs[t] = lp.to(device)\n",
        "        self.not_dones[t] = 1 - torch.tensor(d, dtype=torch.float, device=device)\n",
        "\n",
        "    def calc_returns(self, values, last_value, gamma = GAMMA, gae_lambda = GAE_LAMBDA ):\n",
        "        self.returns = deepcopy(self.rewards.detach())\n",
        "        self.values = deepcopy(values)\n",
        "        last_value = last_value.squeeze()\n",
        "        for t in reversed(range(self.n_steps)):\n",
        "            if t == self.n_steps-1:\n",
        "                delta = self.rewards[t] + gamma * last_value * self.not_dones[t] - self.values[t]\n",
        "                self.advantages[t] = delta\n",
        "            else:\n",
        "                delta = self.rewards[t] + gamma * self.values[t+1] * self.not_dones[t] - self.values[t]\n",
        "                self.advantages[t] = delta + gamma * gae_lambda * self.not_dones[t] * self.advantages[t+1]\n",
        "            self.returns[t] = self.advantages[t] + self.values[t]\n",
        "\n",
        "class GAILTrajData(TrajData):\n",
        "    def __init__(self, n_steps, n_envs, n_obs, n_actions,\n",
        "                 expert_dataset, gail_discriminator):\n",
        "        super().__init__(n_steps, n_envs, n_obs, n_actions)\n",
        "\n",
        "        self.expert_dataset = expert_dataset\n",
        "        self.gail_discriminator = gail_discriminator\n",
        "\n",
        "    def sample_expert_data(self, n):\n",
        "        states, actions = self.expert_dataset.sample_batch(n)\n",
        "        return torch.cat([states, actions], dim=-1)\n",
        "\n",
        "    def update_rewards(self):\n",
        "        if len(self.actions.shape) == 2:\n",
        "            actions = self.actions.unsqueeze(-1)\n",
        "        else:\n",
        "            actions = self.actions\n",
        "        sa = torch.cat([self.states, actions], dim=-1)\n",
        "        self.rewards = self.gail_discriminator.get_rewards(sa).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fYg1AkFYhZz"
      },
      "source": [
        "# **GAIL Discriminator**\n",
        "The **GAIL Discriminator** is a neural network that distinguishes expert trajectories from trajectories generated by the policy. This discriminator acts as a reward function for the agent in adversarial imitation learning.\n",
        "1. **Network Architecture**</br>\n",
        "The discriminator is a neural network that takes state-action pairs $(s,a)$ as input. Outputs a logit score indicating whether the input comes from an expert (1) or from the agent (0).\n",
        "Mathematically, the discriminator is modeled as:\n",
        "$$D_Θ(s,a)=σ(f_Θ(s,a)).$$\n",
        "where:\n",
        "- $f_\\theta(s, a)$ is the neural network output (logits).\n",
        "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
        "\n",
        "2. **Loss Function** </br>\n",
        "The **discriminator loss** is computed using **binary cross-entropy**: $$L_D = \\mathbb{E}_{(s, a) \\sim \\pi} \\left[ \\log(1 - D(s, a)) \\right] + \\mathbb{E}_{(s, a) \\sim \\pi_E} \\left[ \\log D(s, a) \\right]$$ which is implemented as: $$\n",
        "L_D = \\text{BCEWithLogits}(D(s,a), 0) + \\text{BCEWithLogits}(D(s_E,a_E), 1)\n",
        "$$ where:\n",
        "- $\\pi$ is the agent policy.\n",
        "- $\\pi_E$ is the expert policy.\n",
        "- $\\text{BCEWithLogits}$ applies **binary cross-entropy with logits**.\n",
        "\n",
        "3. **Accuracy Metric** </br>\n",
        "The discriminator's **accuracy** is defined as: $$\\text{Accuracy} = \\frac{1}{2} \\left( \\mathbb{E}_{(s_E, a_E)} [D(s_E, a_E) > 0.5] + \\mathbb{E}_{(s, a)} [D(s, a) < 0.5] \\right)$$ which measures how well the discriminator separates expert and agent samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rAu-bn_0fnG"
      },
      "outputs": [],
      "source": [
        "class GAILDiscriminator(torch.nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(GAILDiscriminator, self).__init__()\n",
        "        self.model = make_network(\n",
        "            state_dim + action_dim, 1, device=device\n",
        "        )\n",
        "\n",
        "    def preprocess(self, state, action):\n",
        "        return torch.cat([state, action], dim=-1)\n",
        "\n",
        "    def forward(self, state_action):\n",
        "        shape = state_action.shape\n",
        "        result = self.model(state_action.reshape(-1, shape[-1]))\n",
        "        return result.reshape(*shape[:-1])\n",
        "\n",
        "    def get_rewards(self, state_action):\n",
        "        logits = self.forward(state_action)\n",
        "        return -F.logsigmoid(-logits)\n",
        "\n",
        "    def get_loss(self, traj_data, writer, i):\n",
        "        states, actions = traj_data.states, traj_data.actions\n",
        "        if len(actions.shape) == 2:\n",
        "            actions = actions.unsqueeze(-1)\n",
        "        learner_sa = torch.cat([states, actions], dim=-1)\n",
        "        learner_shape = learner_sa.shape\n",
        "        learner_sa = learner_sa.reshape(-1, learner_shape[-1])\n",
        "        expert_sa = traj_data.sample_expert_data(learner_sa.shape[0])\n",
        "\n",
        "        expert_logits = self.forward(expert_sa)\n",
        "        expert_loss = F.binary_cross_entropy_with_logits(expert_logits, torch.ones_like(expert_logits))\n",
        "\n",
        "        agent_logits = self.forward(learner_sa)\n",
        "        agent_loss = F.binary_cross_entropy_with_logits(agent_logits, torch.zeros_like(agent_logits))\n",
        "\n",
        "        accuracy = ((expert_logits > 0.5).float().mean() + (agent_logits < 0.5).float().mean()) / 2\n",
        "\n",
        "        writer.add_scalar(\"accuracy\", accuracy, i)\n",
        "        return expert_loss + agent_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY3pSGjBcAFC"
      },
      "source": [
        "## **Proximal Policy Optimization (PPO)**\n",
        "In GAIL, the Proximal Policy Optimization (PPO) algorithm is used as the policy optimizer (the generator in the adversarial framework). The generator is responsible for producing trajectories that closely resemble expert demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgciUpWy0iU-"
      },
      "outputs": [],
      "source": [
        "class PPO(nn.Module):\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        super().__init__()\n",
        "        self.name = 'PPO'\n",
        "\n",
        "        torch.manual_seed(SEED)  # needed before network init for fair comparison\n",
        "        self.value = make_network(\n",
        "            n_obs, 1, device=device\n",
        "        )\n",
        "        self.policy = make_network(\n",
        "            n_obs, 2*n_actions, device=device\n",
        "        )\n",
        "\n",
        "    def get_loss(self, traj_data, epsilon=PPO_EPSILON):\n",
        "\n",
        "        # todo: student code here\n",
        "        predicted_values = self.value(traj_data.states).squeeze(-1)\n",
        "        returns = traj_data.returns\n",
        "        loss_fn = nn.MSELoss()\n",
        "        value_loss = loss_fn(predicted_values, traj_data.returns.detach()).mean()\n",
        "        _, probs = self.get_action(traj_data.states)\n",
        "        log_probs = probs.log_prob(traj_data.actions)\n",
        "        old_log_probs = traj_data.log_probs.detach()\n",
        "        ratio = torch.exp(log_probs - old_log_probs)\n",
        "        advantage = traj_data.advantages\n",
        "        clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "        policy_loss = -torch.min(ratio * advantage.detach(), clipped_ratio * advantage.detach()).mean()\n",
        "        loss = value_loss + policy_loss\n",
        "        return loss\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        logits = self.policy(obs)\n",
        "        mean, std = torch.chunk(logits, 2, dim=-1)\n",
        "        mean = torch.tanh(mean)\n",
        "        # probs = categorical.Categorical(logits=logits)\n",
        "\n",
        "        cov_mat = torch.diag_embed(F.softplus(std))#torch.diag(std)#unsqueeze(dim=0)\n",
        "        probs = torch.distributions.MultivariateNormal(mean, cov_mat)\n",
        "        actions = probs.rsample()\n",
        "        return actions, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kzSK3OSedkt"
      },
      "source": [
        "## **Training GAIL**\n",
        "\n",
        "The `GAILRunner` class orchestrates the **training loop for GAIL** by managing trajectory collection, reward computation, and policy updates. In `rollout()`, the agent interacts with the environment using a **PPO policy**, storing states, actions, and log probabilities. Instead of using environment rewards, the **discriminator assigns rewards** based on expert similarity: $$\n",
        "r_{\\text{GAIL}}(s, a) = -\\log \\sigma(-D_\\phi(s, a))\n",
        "$$ where \\( D_\\phi(s, a) \\) is the discriminator’s logit output, and \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\) is the sigmoid function.\n",
        "\n",
        "In `update()`, the **discriminator** is trained using **binary cross-entropy loss** to differentiate between expert and agent-generated trajectories. The **policy (generator)** is optimized using **PPO’s clipped objective**: $$\n",
        "L^{\\text{CLIP}}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
        "$$ where $ r_t(\\theta) $ is the probability ratio of new and old policy actions. By iteratively improving both the **discriminator** and **policy**, `GAILRunner` ensures the agent progressively mimics expert behavior while avoiding direct reliance on environment rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfqoOZxs0ksT"
      },
      "outputs": [],
      "source": [
        "class GAILRunner:\n",
        "    def __init__(self, expert_dataset):\n",
        "        self.n_envs = BATCH_SIZE\n",
        "        self.n_steps = BATCH_SIZE\n",
        "        self.n_obs = N_OBS\n",
        "        self.n_actions = N_ACTIONS\n",
        "\n",
        "        self.envs = gym.make_vec(ENV_NAME, num_envs=self.n_envs, vectorization_mode=\"sync\")\n",
        "\n",
        "        self.learner = PPO(self.n_obs, n_actions=self.n_actions)  # 2 action choices are available\n",
        "\n",
        "        self.discriminator = GAILDiscriminator(self.n_obs, self.n_actions)\n",
        "        self.discriminator_optimizer = Adam(self.discriminator.parameters(), lr=LEARNING_RATE)\n",
        "        self.optimizer = Adam(self.learner.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        self.traj_data = GAILTrajData(self.n_steps, self.n_envs, self.n_obs, n_actions=self.n_actions,\n",
        "                                      expert_dataset=expert_dataset, gail_discriminator=self.discriminator) # 1 action choice is made\n",
        "\n",
        "        self.writer = SummaryWriter(log_dir=f'runs/{self.learner.name}')\n",
        "\n",
        "    def rollout(self, i):\n",
        "        obs, _ = self.envs.reset()\n",
        "        obs = torch.tensor(obs, dtype=torch.float, device=device)\n",
        "\n",
        "        for t in range(self.n_steps):\n",
        "            # PPO doesnt use gradients here, but REINFORCE and VPG do.\n",
        "            with torch.no_grad() if self.learner.name == 'PPO' else torch.enable_grad():\n",
        "                actions, probs = self.learner.get_action(obs)\n",
        "            log_probs = probs.log_prob(actions)\n",
        "            next_obs, rewards, done, truncated, infos = self.envs.step(actions.detach().cpu().numpy())\n",
        "            done = done | truncated  # episode doesnt truncate till t = 500, so never\n",
        "            self.traj_data.store(t, obs, actions, rewards, log_probs, done)\n",
        "            obs = torch.tensor(next_obs,dtype=torch.float,  device=device)\n",
        "        last_value = self.learner.value(obs).detach()\n",
        "        values = self.learner.value(self.traj_data.states).detach().squeeze()\n",
        "        self.writer.add_scalar(\"Reward/original\", self.traj_data.rewards.mean(), i)\n",
        "        self.traj_data.update_rewards()\n",
        "        self.traj_data.calc_returns(values, last_value=last_value)\n",
        "\n",
        "        self.writer.add_scalar(\"Reward/GAIL\", self.traj_data.rewards.clone().detach().mean(), i)\n",
        "        self.writer.flush()\n",
        "\n",
        "    def update(self, i):\n",
        "        learner_epochs = GENERATOR_ITERATIONS\n",
        "        disc_epochs = DISCRIMINATOR_ITERATIONS\n",
        "\n",
        "        disc_losses = []\n",
        "        learner_losses = []\n",
        "        for _ in range(disc_epochs):\n",
        "            disc_loss = self.discriminator.get_loss(self.traj_data, self.writer, i)\n",
        "            self.discriminator_optimizer.zero_grad()\n",
        "            disc_loss.backward()\n",
        "            self.discriminator_optimizer.step()\n",
        "            disc_losses.append(disc_loss.detach().item())\n",
        "\n",
        "        for _ in range(learner_epochs):\n",
        "            learner_loss = self.learner.get_loss(self.traj_data)\n",
        "            self.optimizer.zero_grad()\n",
        "            learner_loss.backward()\n",
        "            self.optimizer.step()\n",
        "            learner_losses.append(learner_loss.detach().item())\n",
        "\n",
        "        self.writer.add_scalar(\"loss/learner_loss\", sum(learner_losses) / len(learner_losses), i)\n",
        "        self.writer.add_scalar(\"loss/disc_loss\", sum(disc_losses) / len(disc_losses), i)\n",
        "        self.writer.flush()\n",
        "        self.traj_data.detach()\n",
        "\n",
        "    def evaluate_policy(self, i, n_eval_episodes = 5):\n",
        "        obs, _ = self.envs.reset()\n",
        "        obs = torch.tensor(obs, dtype=torch.float, device=device)\n",
        "        episode_counts = np.zeros(self.n_envs, dtype=\"int\")\n",
        "        episode_count_targets = np.array([(n_eval_episodes + i) // self.n_envs for i in range(self.n_envs)], dtype=\"int\")\n",
        "        rewardsum_current = np.zeros(self.n_envs)\n",
        "        rewardsum_untildone =[]\n",
        "        dones = np.zeros(self.n_envs, dtype=\"bool\")\n",
        "        while (episode_counts < episode_count_targets).any():\n",
        "            with torch.no_grad() if self.learner.name == 'PPO' else torch.enable_grad():\n",
        "                actions, probs = self.learner.get_action(obs)\n",
        "            next_obs, rewards, done, truncated, infos = self.envs.step(actions.detach().cpu().numpy())\n",
        "            done = done | truncated  # episode doesnt truncate till t = 500, so never\n",
        "            rewardsum_current += rewards\n",
        "            obs = torch.tensor(next_obs, dtype=torch.float, device=device)\n",
        "            for env in range(self.n_envs):\n",
        "                if episode_counts[env] < episode_count_targets[env]:\n",
        "                    if done[env]:\n",
        "                        rewardsum_untildone.append(rewardsum_current[env])\n",
        "                        rewardsum_current[env] = 0\n",
        "                        episode_counts[env] += 1\n",
        "        if rewardsum_untildone:\n",
        "            mean_rewardsum = np.mean(rewardsum_untildone)\n",
        "            std_rewardsum = np.std(rewardsum_untildone)\n",
        "            self.writer.add_scalar(\"Reward/evaluation\", mean_rewardsum, i)\n",
        "        else:\n",
        "            mean_rewardsum = 0\n",
        "            std_rewardsum = 0\n",
        "            self.writer.add_scalar(\"Reward/evaluation\", 0, i)\n",
        "\n",
        "        return mean_rewardsum, std_rewardsum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1TjYa3HfYjH"
      },
      "source": [
        "## **Expert Dataset**\n",
        "The provided code defines and loads an **expert dataset** for training a **GAIL discriminator** or a **Behavior Cloning (BC)** policy. The `ExpertDataset` class stores expert state-action pairs, converting them into PyTorch tensors for efficient sampling. It provides methods for retrieving individual samples and randomly sampling batches for training. The dataset is loaded from a pre-saved file (`HalfCheetah-v2_25.pkl`), extracting the first two expert trajectories of states and actions. The actions are reshaped if necessary to ensure correct dimensionality. The extracted data is then flattened to remove sequence dependencies, making it compatible with neural network training. Finally, an `ExpertDataset` instance is created, which serves as input to the GAIL discriminator (for adversarial training) or the BC policy (for supervised learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnH-nTTV0nLG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ExpertDataset(Dataset):\n",
        "    def __init__(self, states, actions):\n",
        "        super().__init__()\n",
        "        assert len(states) == len(actions)\n",
        "        self.states = torch.from_numpy(states).float().to(device)\n",
        "        self.actions = torch.from_numpy(actions).float().to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.states[idx], self.actions[idx]\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        \"\"\"Sample a batch of states and actions\"\"\"\n",
        "        indices = torch.randint(0, len(self), (batch_size,))\n",
        "        return self.states[indices], self.actions[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Z5i_H1dmDW"
      },
      "outputs": [],
      "source": [
        "# Load the expert data\n",
        "with open('HalfCheetah-v2_25.pkl', 'rb') as f:\n",
        "    expert_dataset = pickle.load(f)\n",
        "\n",
        "num_expert_trajs = 2\n",
        "exp_states = np.stack(expert_dataset['states'][:num_expert_trajs])\n",
        "exp_actions = np.stack(expert_dataset['actions'][:num_expert_trajs])\n",
        "\n",
        "if len(exp_actions.shape) == 2:\n",
        "    exp_actions = np.expand_dims(exp_actions, axis=-1)\n",
        "\n",
        "# Prepare data for ExpertDataset\n",
        "exp_states_flat = exp_states.reshape(-1, exp_states.shape[-1])\n",
        "exp_actions_flat = exp_actions.reshape(-1, exp_actions.shape[-1])\n",
        "\n",
        "# Create ExpertDataset instance\n",
        "expert_dataset = ExpertDataset(exp_states_flat, exp_actions_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "2b3fRQHsdtug",
        "outputId": "534a6e0b-125c-4499-e0e1-2d3ad1ef21fe"
      },
      "outputs": [],
      "source": [
        "gail = GAILRunner(expert_dataset=expert_dataset)\n",
        "\n",
        "# Create enlighten manager for progress tracking\n",
        "manager = enlighten.get_manager()\n",
        "epochs_pbar = manager.counter(total=EPOCHS, desc=\"Training epochs\", unit=\"epochs\")\n",
        "\n",
        "returns_by_epoch = np.empty((0,3))\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    gail.rollout(i)\n",
        "    gail.update(i)\n",
        "    if i%10 == 0:\n",
        "      [mean, std] = gail.evaluate_policy(i)\n",
        "      returns_by_epoch = np.append(returns_by_epoch, np.array([[i, mean, std]]), axis=0)\n",
        "    epochs_pbar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tld1QBbafyFe"
      },
      "source": [
        "## **Behavior Cloning**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cNhGdzB07Ms"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "class BCPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as before, but we’ll keep it here for clarity.\n",
        "    If stochastic=True, outputs (mean, std).\n",
        "    If stochastic=False, outputs just the deterministic action.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_obs, n_actions, stochastic=False):\n",
        "        super().__init__()\n",
        "        self.stochastic = stochastic\n",
        "\n",
        "        torch.manual_seed(SEED)  # needed before network init for fair comparison\n",
        "        self.net = make_network(\n",
        "            n_obs, HIDDEN_DIM, device=device\n",
        "        )\n",
        "\n",
        "        if stochastic:\n",
        "            self.head = nn.Linear(HIDDEN_DIM, 2*n_actions)  # mean + log_std\n",
        "        else:\n",
        "            self.head = nn.Linear(HIDDEN_DIM, n_actions)\n",
        "\n",
        "    def forward(self, states):\n",
        "        x = self.net(states)\n",
        "        out = self.head(x)\n",
        "        if self.stochastic:\n",
        "            mean, log_std = torch.chunk(out, 2, dim=-1)\n",
        "            # We'll do a softplus so std is positive\n",
        "            std = F.softplus(log_std)\n",
        "            return mean, std\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_action(self, obs):\n",
        "        \"\"\"\n",
        "        For env rollout. If stochastic => sample from Normal(mean, std).\n",
        "        If deterministic => just output the direct action from the net.\n",
        "        \"\"\"\n",
        "        if self.stochastic:\n",
        "            mean, std = self.forward(obs)\n",
        "            dist = torch.distributions.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "        else:\n",
        "            action = self.forward(obs)\n",
        "        return action, None  # mimic the signature from PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gH51O695HjY"
      },
      "outputs": [],
      "source": [
        "def train_bc_deterministic(\n",
        "    expert_dataset,\n",
        "    n_obs,\n",
        "    n_actions,\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    run_name=\"BC_Det\"\n",
        "):\n",
        "    # 1) Create policy\n",
        "    policy = BCPolicy(n_obs=n_obs, n_actions=n_actions, stochastic=False).to(device)\n",
        "    optimizer = Adam(policy.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 2) Create DataLoader\n",
        "    loader = DataLoader(expert_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 3) Create TensorBoard writer\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "\n",
        "    # 4) Training Loop\n",
        "    global_step = 0\n",
        "    policy.train()\n",
        "\n",
        "    # Create enlighten manager for progress tracking\n",
        "    manager = enlighten.get_manager()\n",
        "    epochs_counter = manager.counter(total=n_epochs, desc=\"Epochs\", unit=\"epochs\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Create counter for batches within this epoch\n",
        "        batches_counter = manager.counter(\n",
        "            total=len(loader),\n",
        "            desc=f\"Epoch {epoch+1} batches\",\n",
        "            unit=\"batches\",\n",
        "            leave=False\n",
        "        )\n",
        "\n",
        "        for batch_i, (s, a) in enumerate(loader):\n",
        "            s = s.to(device)\n",
        "            a = a.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = policy(s)                  # shape [B, action_dim]\n",
        "            loss = F.mse_loss(pred, a)        # MSE for continuous deterministic BC\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log to TensorBoard\n",
        "            writer.add_scalar(\"loss/mse\", loss.item(), global_step)\n",
        "            global_step += 1\n",
        "\n",
        "            # Update batch counter\n",
        "            batches_counter.update()\n",
        "\n",
        "        # Update epoch counter\n",
        "        batches_counter.close()\n",
        "        epochs_counter.update()\n",
        "\n",
        "    writer.close()\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVa3UFIl4Pme"
      },
      "outputs": [],
      "source": [
        "def train_bc_stochastic(\n",
        "    expert_dataset,\n",
        "    n_obs,\n",
        "    n_actions,\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    run_name=\"BC_Stoch\"\n",
        "):\n",
        "    # 1) Create policy\n",
        "    policy = BCPolicy(n_obs=n_obs, n_actions=n_actions, stochastic=True).to(device)\n",
        "    optimizer = Adam(policy.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 2) Create DataLoader\n",
        "    loader = DataLoader(expert_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 3) Create TensorBoard writer\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "\n",
        "    # 4) Training Loop\n",
        "    global_step = 0\n",
        "    policy.train()\n",
        "\n",
        "    # Create enlighten manager for progress tracking\n",
        "    manager = enlighten.get_manager()\n",
        "    epochs_counter = manager.counter(total=n_epochs, desc=\"Epochs\", unit=\"epochs\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Create counter for batches within this epoch\n",
        "        batches_counter = manager.counter(\n",
        "            total=len(loader),\n",
        "            desc=f\"Epoch {epoch+1} batches\",\n",
        "            unit=\"batches\",\n",
        "            leave=False\n",
        "        )\n",
        "\n",
        "        for batch_i, (s, a) in enumerate(loader):\n",
        "            s = s.to(device)\n",
        "            a = a.to(device)\n",
        "\n",
        "            # Forward pass => (mean, std)\n",
        "            mean, std = policy(s)\n",
        "            dist = torch.distributions.Normal(mean, std)\n",
        "\n",
        "            # Negative log-likelihood of the expert actions\n",
        "            loss = -dist.log_prob(a).mean()\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log to TensorBoard\n",
        "            writer.add_scalar(\"loss/nll\", loss.item(), global_step)\n",
        "            global_step += 1\n",
        "\n",
        "            # Update batch counter\n",
        "            batches_counter.update()\n",
        "\n",
        "        # Update epoch counter\n",
        "        batches_counter.close()\n",
        "        epochs_counter.update()\n",
        "\n",
        "    writer.close()\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyPU5wYS4-fa"
      },
      "outputs": [],
      "source": [
        "# Train Deterministic BC\n",
        "bc_deterministic = train_bc_deterministic(\n",
        "    expert_dataset=expert_dataset,\n",
        "    n_obs=N_OBS,\n",
        "    n_actions=N_ACTIONS,\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    run_name=\"BC_Det\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StWVKUpX_GsF"
      },
      "outputs": [],
      "source": [
        "# Train Stochastic BC\n",
        "bc_stochastic = train_bc_stochastic(\n",
        "    expert_dataset=expert_dataset,\n",
        "    n_obs=N_OBS,\n",
        "    n_actions=N_ACTIONS,\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    run_name=\"BC_Stoch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbrbRfAg7-cH"
      },
      "source": [
        "## Visualization and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8r-YUugpd1S"
      },
      "outputs": [],
      "source": [
        "# @title Policy visualization / demo code.\n",
        "\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import Video, display, clear_output\n",
        "\n",
        "def visualize(agent, policy_name=\"agent\", env_name=ENV_NAME):\n",
        "    video_dir = \"./videos\"  # Directory to save videos\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "    # Create environment with proper render_mode\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "\n",
        "    # name_prefix ensures each video file is distinct\n",
        "    env = RecordVideo(\n",
        "        env,\n",
        "        video_folder=video_dir,\n",
        "        episode_trigger=lambda e_id: True,\n",
        "        name_prefix=policy_name  # This gets appended to \"rl-video-step-...\"\n",
        "    )\n",
        "\n",
        "    obs, _ = env.reset(seed=SEED)\n",
        "\n",
        "    for t in range(1000):\n",
        "        # Get action from policy\n",
        "        actions, _ = agent.get_action(\n",
        "            torch.tensor(obs, dtype=torch.float, device=device)[None, :]\n",
        "        )\n",
        "        obs, _, done, truncated, _ = env.step(actions.cpu().detach().numpy()[0])\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # The RecordVideo wrapper names the file automatically with the prefix + step info\n",
        "    # We'll grab the latest video with our given prefix\n",
        "    # e.g. \"agent_rl-video-episode-0.mp4\" or similar\n",
        "    filtered_videos = sorted(\n",
        "        f for f in os.listdir(video_dir)\n",
        "        if f.endswith(\".mp4\") and policy_name in f\n",
        "    )\n",
        "    if len(filtered_videos) == 0:\n",
        "        logger.warning(\"No videos found!\")\n",
        "        return\n",
        "\n",
        "    video_path = os.path.join(video_dir, filtered_videos[-1])  # the newest file\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(Video(video_path, embed=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUNNmT8A5BcO"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Visualizing GAIL Policy...\")\n",
        "visualize(gail.learner, policy_name=\"GAIL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF-nVhNT5SRR"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Visualizing Deterministic BC Policy...\")\n",
        "visualize(bc_deterministic, policy_name=\"BC_Deterministic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oypdTGM0J0sM"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Visualizing Stochastic BC Policy...\")\n",
        "visualize(bc_stochastic, policy_name=\"BC_Stochastic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqJSYQp6EU9O"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate policies and compare\n",
        "\n",
        "def collect_eval_data(\n",
        "        agent,\n",
        "        env_name=ENV_NAME,\n",
        "        n_episodes=EVAL_EPISODES\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Runs 'n_eval_episodes' rollouts in the given env_name, returning list of episode returns.\n",
        "    \"\"\"\n",
        "    agent.eval()\n",
        "    returns = []\n",
        "\n",
        "    # Create a progress bar for evaluation episodes\n",
        "    manager = enlighten.get_manager()\n",
        "    eval_pbar = manager.counter(total=n_episodes, desc=\"Evaluating\", unit=\"episodes\")\n",
        "\n",
        "    for episode_i in range(n_episodes):\n",
        "        env = gym.make(env_name)\n",
        "\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        total_reward = 0.0\n",
        "        terminated, truncated = False, False\n",
        "        while not (terminated or truncated):\n",
        "            # Wrap obs in a torch tensor\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float, device=device).unsqueeze(0)\n",
        "            action, _ = agent.get_action(obs_tensor)\n",
        "            # Convert action back to numpy for the env\n",
        "            obs, reward, terminated, truncated, _ = env.step(action.detach().cpu().numpy()[0])\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "        env.close()\n",
        "\n",
        "        # Update progress bar\n",
        "        eval_pbar.update()\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnq5QoZReDfi"
      },
      "outputs": [],
      "source": [
        "# Collect evaluation data for each policy\n",
        "logger.info(\"Collecting evaluation data.\")\n",
        "gail_returns = collect_eval_data(gail.learner)\n",
        "bc_det_returns = collect_eval_data(bc_deterministic)\n",
        "bc_stoch_returns = collect_eval_data(bc_stochastic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQCFBe5yeDsd"
      },
      "outputs": [],
      "source": [
        "# Get summary evaluation statistics\n",
        "gail_mean, gail_std = float(np.mean(gail_returns)), float(np.std(gail_returns))\n",
        "bc_det_mean, bc_det_std = float(np.mean(bc_det_returns)), float(np.std(bc_det_returns))\n",
        "bc_stoch_mean, bc_stoch_std = float(np.mean(bc_stoch_returns)), float(np.std(bc_stoch_returns))\n",
        "\n",
        "logger.info(f\"GAIL policy:     return={gail_mean:.1f} ± {gail_std:.1f}\")\n",
        "logger.info(f\"BC Deter policy: return={bc_det_mean:.1f} ± {bc_det_std:.1f}\")\n",
        "logger.info(f\"BC Stoch policy: return={bc_stoch_mean:.1f} ± {bc_stoch_std:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLQ24XsuJl7D"
      },
      "outputs": [],
      "source": [
        "#@title Plot returns for all policies\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "FS_TICK: int = 12\n",
        "FS_LABEL: int = 18\n",
        "PLOT_DPI: int=1200\n",
        "PLOT_FORMAT: str='pdf'\n",
        "RC_PARAMS: dict = {\n",
        "    # Set background and border settings\n",
        "    'axes.facecolor': 'white',\n",
        "    'axes.edgecolor': 'black',\n",
        "    'axes.linewidth': 2,\n",
        "    'xtick.color': 'black',\n",
        "    'ytick.color': 'black',\n",
        "}\n",
        "\n",
        "def plot_returns(data, xlabel, ylabel, title):\n",
        "    plt.rcParams.update(RC_PARAMS);\n",
        "    sns.set_palette('deep')\n",
        "    fig = plt.figure()\n",
        "    ax = sns.boxplot(\n",
        "        data=data,\n",
        "        palette='deep',\n",
        "        orient='v',\n",
        "        showmeans=True,\n",
        "        meanprops={\n",
        "            'markerfacecolor': 'white',\n",
        "            'markeredgecolor': 'black'\n",
        "            },\n",
        "        flierprops={'marker': 'x'}\n",
        "    )\n",
        "\n",
        "    # sns.stripplot(\n",
        "    #     data=data,\n",
        "    #     alpha=0.5,\n",
        "    #     color=\"black\",\n",
        "    #     jitter=True\n",
        "    # )\n",
        "\n",
        "    plt.xlabel(xlabel, fontsize=FS_LABEL)\n",
        "    plt.ylabel(ylabel, fontsize=FS_LABEL)\n",
        "    plt.yticks(fontsize=FS_TICK)\n",
        "    plt.xticks(fontsize=FS_TICK)\n",
        "\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    fig.savefig(f\"{title}.{PLOT_FORMAT}\", dpi=PLOT_DPI, format=PLOT_FORMAT)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "def plot_returns_by_epoch(data, xlabel, ylabel, title):\n",
        "  plt.rcParams.update(RC_PARAMS);\n",
        "  sns.set_palette('deep')\n",
        "  fig = plt.figure()\n",
        "  xdata = data['epoch']\n",
        "  ydata = data['return_mean']\n",
        "  ax = sns.lineplot(\n",
        "      x=xdata,\n",
        "      y=ydata,\n",
        "      palette='deep',\n",
        "      orient='v',\n",
        "      showmeans=True,\n",
        "      meanprops={\n",
        "          'markerfacecolor': 'white',\n",
        "          'markeredgecolor': 'black'\n",
        "          },\n",
        "      flierprops={'marker': 'x'}\n",
        "  )\n",
        "\n",
        "\n",
        "  plt.xlabel(xlabel, fontsize=FS_LABEL)\n",
        "  plt.ylabel(ylabel, fontsize=FS_LABEL)\n",
        "  plt.yticks(fontsize=FS_TICK)\n",
        "  plt.xticks(fontsize=FS_TICK)\n",
        "\n",
        "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()\n",
        "  fig.savefig(f\"{title}.{PLOT_FORMAT}\", dpi=PLOT_DPI, format=PLOT_FORMAT)\n",
        "  plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0WeAp2TeKhi"
      },
      "outputs": [],
      "source": [
        "return_data = {\n",
        "    \"GAIL\": gail_returns,\n",
        "    \"BC Deterministic\": bc_det_returns,\n",
        "    \"BC Stochastic\": bc_stoch_returns\n",
        "}\n",
        "plot_returns(\n",
        "    return_data,\n",
        "    xlabel='Policy',\n",
        "    ylabel='Return',\n",
        "    title='Evaluation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "return_by_epoch_data = {\n",
        "    \"epoch\": returns_by_epoch[:,0],\n",
        "    \"return_mean\": returns_by_epoch[:,1],\n",
        "    \"return_std\": returns_by_epoch[:,2]\n",
        "}\n",
        "\n",
        "plot_returns_by_epoch(\n",
        "    return_by_epoch_data,\n",
        "    xlabel='Epoch',\n",
        "    ylabel='Return',\n",
        "    title='Evaluation by Epochs'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTSOtYWK-gFj"
      },
      "source": [
        "***"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gail",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
